---
title: 统计学相关问题总结
date: 2023-04-20 17:42:09 +0800
categories: [数据分析,统计学]
tags: [统计学]
math: true
mermaid: true
---

最近总结了一些日常分析及学习过程中常见的统计学问题。

## 常见的概率分布模型

常见的概率分布模型主要分为离散型和连续型两种，其中离散型分布包括0-1分布、二项分布、几何分布、泊松分布；连续型分布包括均匀分布、指数分布、正态分布。

接下来具体介绍一下常见分布。

1. 0-1分布
    + 0-1分布也成为伯努利分布，任何一个只有两种结果的随机现象都服从0-1分布。

2. 二项分布
    + 在n次独立重读的伯努利实验中，设每次试验中事件A发生的概率为p。用X表示n重伯努利试验中事件A发生的次数，则X的可能取值为0，1，…，n,且对每一个k（0≤k≤n）,事件{X=k}即为“n次试验中事件A恰好发生k次”，随机变量X的分布即为二项分布。
    + 当二项分布的n很大而p很小时，泊松分布可作为二项分布的近似，其中λ为np。通常当n≧20,p≦0.05时，就可以用泊松公式近似得计算。事实上，泊松分布正是由二项分布推导而来的。

3. 几何分布
    + 在n次伯努利试验中，试验k次才得到第一次成功的机率。详细地说，是：前k-1次皆失败，第k次成功的概率。几何分布是帕斯卡分布当r=1时的特例。

4. 泊松分布
    + 当一个事件满足如下条件时，可以认为这个事件在某一固定时间段内发生次数满足泊松分布。
    
         （1）事件是独立发生的。

         （2）事件发生的概率在给定的固定时间内不随时间变化，也就是随机独立发生，x表示一段时间内事件发生的次数，$\lambda $ 表示一段时间内事件发生的平均次数。

    + 曾经在学习运筹学时，在排队论中常见泊松分布。

5. 均匀分布
    + 也称为矩形分布，在相同长度间隔的分布概率是相等的。

6. 指数分布
    + 可以用来表示独立随机事件发生的时间间隔，比如旅客进机场的时间间隔。
    + 指数函数的一个重要特征是无记忆性。这表示如果一个随机变量呈指数分布，当s,t>0时有P(T>t+s|T>t)=P(T>s)。即，如果T是某一元件的寿命，已知元件使用了t小时，它总共使用至少s+t小时的条件概率，与从开始使用时算起它使用至少s小时的概率相等。

7. 正态分布
    + 正态分布可以说是日常学习中最常用的分布了，相加减后仍然服从正态分布（期望相加减，方差相加）
    + k个独立的标准正态分布随机变量的平方和构成卡方分布，在假设检验及拟合优度检验（一个样本是否来自于一个特定的理论分布）方面常用。
    

## 常见的分布变换方法

数据变换常常用于进行t检验等假设检验之前使数据更接近正太分布。通过适当的数据变换，可以减小或消除数据的偏斜和异方差性，从而提高t检验的可靠性。主要方式有对数变换、方根变换、Box-Cox变换等。

需要注意的是，数据变换可能会改变数据点额解释与可解释性，因此需要谨慎考虑。此外，如果数据经过变换，确保在报告结果时提供原始和变换后的数据的相关信息，以便其他人理解分析过程。

接下来介绍这三种常见的数据变换方法：

1. 对数变换：
    + 具体是将数据取对数，通常以自然对数或以10为底的对数。
    + 适用于右偏数据。
    + 这种变换有助于拉进数据点，减小极端值的影响，使数据更接近正态分布。

2. 方根变换：
    + 适用于数值较小的右偏数据。

3. Box-Cox变换：
    + 适用Scipy库中的scipy.stats.boxcox()函数进行Box-Cox变换。boxcox()函数的语法如下：
        + scipy.stats.boxcox(x,lambda-None, alpha-None)
        + 参数说明：
        + x: 要进行变换的数据，可以是一维数组或类似的序列。
        + lambda: 可选参数，勇于指定Box-Cox变换的指数。如果不提供该参数，函数会自动计算最佳的lambda值以使数据最接近正态分布。
        + alpha: 可选参数，用于计算置信区间。默认情况下，alpha设为None，不计算置信区间。

4. 反变换：
    + 在进行假设检验后，如果对数据进行了变换，通常需要对检验结果进行反变换，以便在原始数据上进行解释。反变换的过程与所使用的变换方法相对应。

## 假设检验

假设检验的基本思想是“小概率事件”的原理，其统计推断方法是带有某种概率性质的反证法。小概率思想是指小概率事件在一次实验中基本不会发生。反证法思想是先提出检验假设，再用适当的统计方法，利用小概率原理，确定假设是否成立。即为了检验一个假设H0是否正确，首先假定H0正确，然后根据样本对假设H0做出接受或拒绝的决策。如果样本观察值导致了“小概率事件”发生，就应该拒绝假设H0，否则应该接受假设H0。

1. 第一类错误与第二类错误：
    + 第一类错误：H0为真但拒绝了H0。
    + 第二类错误：H1为真但接受了H0。
    + 显著性水平：
        + 这个量描述了在0假设为真的情况下，犯第一类错误的概率$\alpha$，也是我么在假设检验中得到p值之后进行对比的一个标准。这个量也描述了我们的样本是否具有可以代表总体的能力（显著性）。
        + 这个量通常设置为5%、1%等，设定值是我们给出的一个检验标准。具体选取取决于犯两类错误产生的结果的严重性。
        + 显著性水平和p值不是一个东西，注意不要混淆了。它们都是用于判断实验结果是否显著的指标。首先，P值是一个概率值，它表示在原假设为真的情况下，观察到当前实验结果或更极端结果的概率。换句话说，P值描述了实验数据与原假设之间的差异程度。P值越小，说明在原假设下观察到当前实验结果的概率越低，也就意味着实验数据与原假设越不一致。显著性水平则是一个预先设定的标准，用于判断P值是否小到足以拒绝原假设。显著性水平通常用α表示，常见的取值有0.01、0.05或0.1等。这个标准决定了我们愿意接受的风险水平，即在假设检验中错误地拒绝正确原假设的概率。

2. T检验与Z检验：
    + Z检验：
        + Z检验是一般用于大样本（样本容量大于30）平均值差异性检验的方法，它是用标准正态分布的理论来推断差异发生的概率，从而比较两个平均数的差异是否显著。普通正态分布的的标准化计算公式就是计算Z分数，算出来的就叫Z分布。
        + 要求：观测到的数据是正态分布，并且已知总体的标准差。
        + Z检验通常用于以下几个场景：1.检验两个总体的均值或比例之间是否存在显著差异。2.检验样本均值或比例与已知的总体均值或者比例之间是否存在显著差异。3.分析某一变量的变化是否显著。
    + T检验：
        + T检验是基于T分布的，是使用T分布理论来推导差异发生的概率，从而比较两个平均数的差异是否显著。T分布形状与正态分布相似，但是尾部较厚。随着样本量的增加，T分布逐渐接近正态分布。
        + T检验同样要求总体符合正态分布。而不同于Z检验，T检验不要求知道总体标准差，而是使用样本标准差来代替总体标准差。且T分布更适用于小样本（样本量小于30）。
        + T检验通常用于以下几个场景：1.比较两个独立样本的均值。2.比较两个配对样本的均值差异。3.验证样本均值与已知的总体均值之间是否存在显著差异。4.分析某一变量的变化是否显著。
        + 双总体T检验是检验两个样本平均数与其各自所代表的总体的差异是否显著。双总体T检验又分为两种情况，一是独立样本T检验，一是配对样本T检验，所使用的统计量有所不同。
        + stata中应用T检验直接使用ttest函数即可。
    + 区别：
        + 在已知总体标准差的情况下，使用Z检验。在未知总体标准差的情况下，使用T检验。
        + T检验基于T分布，Z检验基于标准正态分布。随着样本量增加，T分布逐渐接近正态分布。
        + 在样本量较小（通常小于30）且总体分布近似正态时，T检验比Z检验更为适用。在样本量较大时，T检验和Z检验的结果通常非常接近。
        + t分布是z分布的小样本分布，即当总体符合z分布时，从总体中抽取的小样本符合t分布，而对于符合t分布的变量，当样本量增大时，变量数据逐渐向z分布趋近。
        + z检验和t检验都是均值差异检验方法，但t分布具有随着样本量增大逐渐逼近z分布的特点，所以t检验的运用要比z检验更广泛。因为大小样本时都可以用t检验，而小样本时z检验不适用。SPSS里面只有t检验，没有z检验的功能模块。    

3. 方差分析：
    + 方差分析（ANOVA）是一种统计方法，用于研究一个或多个分类自变量对一个连续因变量的影响。它比较不同来源的变异对总变异的贡献大小，从而确定可控因素对研究结果影响力的大小。方差分析研究的是不同组或类别之间因变量的均值是否存在显著差异。
    + ANOVA主要有两种类型：
        1. 单因素ANOVA：用于比较一个因素在不同水平下的效应。例如，研究不同教学方法对学生成绩的影响。
        2. 多因素ANOVA：用于同时考虑两个或多个因素。例如，研究教学方法和学生背景对学生成绩的联合影响。
    + ANOVA的核心目标是区分数据变异性的来源：是由实验条件（或处理）的不同引起的，还是由随机变异（即自然波动或实验误差）引起的。这种区分有助于我们判断实验条件是否对研究变量有显著影响。具体来说，如果组间变异远大于组内变异，则说明不同处理或条件对因变量有显著影响；反之，则说明影响不显著。
    + 方差分析的用途：
        1. 比较不同处理或条件下的效果：例如，在农业试验中比较不同肥料对作物产量的影响。
        2. 评估多因素的作用：分析多个因素（如性别、年龄、教育水平等）对某个结果（如收入）的联合影响。
        3. 优化实验设计：通过方差分析，可以确定哪些因素对实验结果有显著影响，从而优化实验设计，减少不必要的变量或增加重要的变量。
    + 前提假设即相应检验方法：
        1. 正态性：各总体分布应为正态分布。
            + 图示法：
                + Q-Q图：通过将样本数据与标准正态分布的分位数进行比较，可以直观地检查数据的正态性。
                + P-P图：与QQ图类似，但使用的是累积分布函数而不是分位数。
                + 直方图：可以展示数据的分布情况，但不如QQ图和PP图直观。
                + 核密度曲线：通过平滑数据点的分布来估计概率密度函数，有助于判断数据的正态性。
                + 箱线图、茎叶图、小提琴图等也可以提供关于数据分布的信息。
            + 统计检验：
                + K-S检验：用于检验单样本是否来自正态分布。
                + S-W检验：同样是检验单样本正态性的方法，对于小样本通常比K-S检验更敏感。
        2. 方差齐性：各总体的方差应相等。
            + 图示法：
                + 箱线图：通过比较不同组别数据的箱子大小（即四分位距），可以初步判断方差是否齐性。
                + 残差图：绘制残差（观测值与预测值之差）与预测值的散点图，观察残差是否随机分布且大小相近。
            + 统计检验：
                + Levene检验：一种常用的方差齐性检验方法，对离群值不太敏感。
                + Hartley检验：适用于小样本的方差齐性检验。
                + Bartlett检验：用于检验多个样本的方差是否相等。
                + 修正的Bartlett检验：对Bartlett检验的改进，适用于某些特定情况。
                + BF检验（Brown-Forsythe检验）：基于转换后的数据进行方差齐性检验，适用于不满足正态性假定的情况。
        3. 独立性：观测值应相互独立。
            + 检验观测值的独立性通常需要根据实验设计和数据收集过程来判断。例如，在随机化实验中，如果每个样本都是从总体中随机抽取的，并且没有重复测量或配对观测，那么可以认为观测值是独立的。此外，也可以通过检查数据中的相关性或时间序列分析等方法来评估观测值的独立性。
    + ANOVA的关键组成部分：
        + 总方差：数据总体的方差，包括组间方差和组内方差。
        + 组间方差：不同组（或处理条件）间均值的差异。
        + 组内方差：同一组内个体间的差异。
        通过比较组间方差和组内方差，ANOVA帮助我们判断各组间是否存在显著的均值差异。如果组间方差显著大于组内方差，我们有理由相信不同组的均值存在显著差异。
    + ANOVA的步骤：
        + 确定假设：
            + 零假设（H0）：所有组的均值相等，即组间不存在显著差异。
            + 备择假设（H1）：至少有两组的均值不等，即存在至少一个组间的显著差异。
        + 计算组间和组内方差：
            + 组间方差：计算每个组的均值与总体均值之间的差异，反映了不同处理或条件下数据的变化程度。
            + 组内方差：计算组内数据点与各自组均值的差异，表示在相同条件下的数据波动。
        + 计算F值：
            + F值是方差分析中的核心统计量，它是组间方差与组内方差的比率：F = 组间方差 / 组内方差。
            + 较高的F值通常表明组间存在显著差异。但我们需要通过F分布来确定这个差异是否统计上显著。
        + 查找临界值并作出结论：
            + 我们可以通过F分布表或相关软件，根据自由度和显著性水平（通常是0.05）找到F值的临界值。如果计算出的F值超过临界值，我们拒绝零假设，认为至少有两组间存在显著差异。
    + 要检验多个总体均值是否相等时，为什么不作两两比较，而用方差分析方法？
        + 效率问题：如果直接进行两两比较，需要进行的比较次数会随着总体数量的增加而迅速增加。这不仅增加了计算量，还可能因为多次比较而增加第一类错误（即错误地拒绝原假设）的概率。
        + 信息利用不足：两两比较只考虑了每对总体之间的差异，没有充分利用所有数据的信息。而方差分析则综合考虑了所有数据，能够更全面地评估不同总体之间的差异。
        + 交互作用的理解：当存在多个自变量时，两两比较无法有效地分析这些自变量之间的交互作用。而方差分析则可以通过设置不同的处理组合，揭示自变量之间的交互效应。
    + 多重比较的方法和思想总结（适合场所、思路、通俗解释）：
        + 多重比较是在方差分析后，进一步比较各总体均值之间差异的方法。当方差分析显示至少有一个组与其他组存在显著差异时，多重比较可以帮助我们确定具体是哪些组之间存在差异。
        + LSD法（最小显著差异法）：
            + 适合场所：适用于各组样本量相等或近似相等的情况。
            + 思路：计算各均值之间的差值，并与一个临界值（最小显著差异）进行比较。
            + 通俗解释：类似于“打擂台”，看哪两个均值之间的差异大到足够让人认为它们是不一样的。
        + HSD法（Tukey-Kramer方法）：
            + 适合场所：对样本量不等的情况也适用，且考虑了误差项。
            + 思路：根据组间的自由度和组内误差的均方，计算出一个调整后的临界值，用于比较均值差异。
            + 通俗解释：这种方法更“公平”，它考虑了不同组之间可能存在的不同误差，然后给出了一个更准确的“打分标准”。
        + SNK法：
            + 适合场所：适用于预先设定好的几个组之间的比较。
            + 思路：根据设定的组别，进行特定的均值比较。
            + 通俗解释：类似于“分组比赛”，只比较我们关心的那几组之间的均值差异。
        + Dunnet-t法：
            + 适合场所：常用于与对照组进行比较的多个实验组的情况。
            + 思路：将每个实验组与对照组进行比较，控制犯第一类错误的概率。
            + 通俗解释：类似于“打擂台”，但这次是每个实验组都要与“冠军”（对照组）比较一下，看谁更厉害。
        + Sidak法和Bonferroni法：
            + 适合场所：适用于所有可能的均值对比较。
            + 思路：通过调整临界值来控制犯第一类错误的总概率。
            + 通俗解释：这两种方法都是“保守派”，它们通过提高判断“两个均值不同”的标准，来减少误判的可能性。
        + Scheffe法：
            + 适合场所：适用于所有线性组合的均值比较。
            + 思路：构建一个线性组合的置信区间，用于比较均值。
            + 通俗解释：这种方法考虑的是均值的“组合效应”，而不是单独看两个均值之间的差异。
        + Duncan法：
            + 适合场所：当各组样本量不等时，且需要比较所有可能的均值对。
            + 思路：通过特定的算法调整临界值，以适用于不等样本量的情况。
            + 通俗解释：这种方法考虑到了不同组之间的“实力差异”（即样本量不同），并给出了相应的比较标准。


## 相关分析 

相关分析是用于探索和量化两个或多个变量之间关系的方法。

1. 皮尔逊相关系数：
    + 简介：pearson法则是一种经典的相关系数计算方法，主要用于表征线性相关性，值介于-1与1之间，其绝对值越接近于1，表明这两个变量之间的相关程度越高，即这两个变量越相似。
    + 公式：具体公式就是将两个样本观测值与其均值的乘积求和，再比上两个样本观测值与其均值差的几何平均数的乘积。如下：
    $$ r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2} \sqrt{\sum_{i=1}^n (Y_i - \bar{Y})^2}} $$
    + 要求：线性关系。

2. 斯皮尔曼的原理：
    + Spearman 相关性分析是对两组变量的等级大小作相关性分析，从而得到一个自变量与因变量之间的关系和自变量对因变量的影响强弱。它首先将两组变量的数据按照大小顺序排列，然后用等级代替原始数据，最后计算等级之间的相关性。
    + 公式：设自变量 X 和 Y 的 2 个随机样本为 ( x1 ,y1 ),⋯,( xn ,yn )，将 x1 ,⋯,xn和 y1 ,⋯,yn按升序方式进行排列，则X和Y的spearman秩相关系数计算公式如下，其中ri为xi的秩，si为yi的秩，r、s分别为秩的平均值：
    $$ \rho = \frac{\sum_{i=1}^{n} (r_i - \bar{r})(s_i - \bar{s})}{\sqrt{\sum_{i=1}^{n} (r_i - \bar{r})^2} \sqrt{\sum_{i=1}^{n} (s_i - \bar{s})^2}} $$
    + 要求：单调性。
    + 斯皮尔曼与皮尔逊的比较：
        + Pearson 适用于两个变量之间的线性关系，而Spearman适用于单调关系。
        + Pearson 处理变量的数据原始值，而 Spearman 处理数据排序值（需要先做变换，transform）
        + 如果散点图表明“可能是单调的，可能是线性的”关系，最好的选择是 Spearman 而不是 Pearson。即使数据证明是完全线性的，用 Spearman 也不会造成信息丢失。但是，如果不是完全线性但使用 Pearson 系数，会丢失 Spearman 可以捕获的信息，是否单调。

3. 协方差与相关系数：
    1. 协方差：协方差（Covariance）用于衡量两个变量的总体误差。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。
        + 注：协方差可以反应两个变量的协同关系， 变化趋势是否一致。同向还是方向变化。
    2. 相关系数：相关系数是用以反映变量之间相关关系密切程度的统计指标。
        + 注：相关系数是协方差的归一化(normalization)， 消除了两个变量量纲/变化幅度不同的影响。单纯反映两个变量在每单位变化的相似程度。

4. 相关分析与回归分析的区别：
    + 相关分析只能反映变量之间是否存在关联，以及关联的方向和程度，但不能说明变量之间的因果关系。回归分析则可以反映变量之间的因果关系，以及因变量如何受到自变量的影响。
    + 相关分析中涉及的变量不存在自变量和因变量的划分，变量之间的关系是对等的。回归分析中则必须根据研究对象的性质和分析的目的，对变量进行自变量和因变量的划分，变量之间的关系是不对等的。
    + 相关分析中所有的变量都必须是随机变量，即变量的值是由随机因素决定的，不能人为控制。回归分析中，自变量可以是确定的，即变量的值是由人为控制的，也可以是随机的，因变量则必须是随机的，即变量的值是由自变量和随机误差共同决定的。



